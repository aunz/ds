{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aesop's Fables",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "E2MumWS1Ld63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Aesop's Fables\n",
        "AI to learn Aesop's Fables and create more stories\n",
        "\n",
        "![Aesop's Fables](https://images-na.ssl-images-amazon.com/images/I/51DM5uYLhZL._SX258_BO1,204,203,200_.jpg)\n",
        "\n",
        "\n",
        "The model is built using a neural network with a LSTM (long short term memory) layer. The data are text from Aesop's fables. It's a relatively small dataset of only 188540 characters.\n",
        "\n",
        "**Live demo: https://laesop-fables-generator.glitch.me/**"
      ]
    },
    {
      "metadata": {
        "id": "LfrKBWOD3s4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B7uSiJz-2qmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "519ccebf-cb01-4190-ebda-5c12901bd7cf"
      },
      "cell_type": "code",
      "source": [
        "# get the data\n",
        "data = __import__('requests').get('https://raw.githubusercontent.com/aunz/ds/master/.data/Aesops_Fables.txt').text[:-50]\n",
        "print('Corpus length', len(data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length 188540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "atAwhYhs89iN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a6b814a-a537-41b6-f93c-aa9f2c84e87a"
      },
      "cell_type": "code",
      "source": [
        "# extract the data into sentences and next_chars\n",
        "sentences = [] # store the extracted sequences\n",
        "next_chars = [] # store the target (the follow-up characters)\n",
        "max_len = 60 # extract sequences of 60 characters\n",
        "step = 2 # sample a new sequence every two characters\n",
        "\n",
        "for i in range(0, len(data) - max_len, step):\n",
        "    sentences.append(data[i: i + max_len])\n",
        "    next_chars.append(data[i + max_len])\n",
        "\n",
        "print('Number of sequences:', len(sentences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 94240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gXnVRaWmAU-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1a6c3306-bf83-495c-8365-821ffd2c1662"
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(data))) # list of unique char in the corpus\n",
        "print('Unique characters:', len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars) # turn the list into dict to map unique chars to their index in the list \"chars\"\n",
        "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences): # one hot encode into binary arrays\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: 64\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N6BYP4y5Dooa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "524f62bc-df1e-4197-d837-4740242f6758"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# a simple model of LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_len, len(chars)), dropout=0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.01))\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 107,072\n",
            "Trainable params: 107,072\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SJxQI_YjGIqm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=0.5):\n",
        "    ''' function to sample the next char given the model's prediction\n",
        "    Because given the same input, the predicted output will always be the same.\n",
        "    This function take an array of preds (sum to 1), and a temperature param, return a new array of preds, then randomly select 1 element\n",
        "    At a low temp, the new array of preds has a distribution similar to the old array\n",
        "    Conversely, at a high temp, the new array of preds will be quite different from the old array\n",
        "    Based on the new distribution of pred probability, the numpy np.random.multinomial is used to randomly select 1 element\n",
        "    '''\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log1p(preds) / temperature\n",
        "    preds = np.expm1(preds)\n",
        "    preds = preds / np.sum(preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def predict(text_seed, n_char=420, temperature=0.5):\n",
        "    # given the seed_text, predict up to the next n_char\n",
        "    generated_text = f'{text_seed:>60}'[:max_len] # pad string to the right and only take up to max_len\n",
        "    for i in range(n_char):\n",
        "        sampled = np.zeros((1, max_len, len(chars))) # one hot encodes the chars generated so far\n",
        "        for t, char in enumerate(generated_text):\n",
        "            if char in char_indices: sampled[0, t, char_indices[char]] = 1.\n",
        "        preds = model.predict(sampled, verbose=0)[0] # sample the next char\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = chars[next_index]\n",
        "        generated_text += next_char\n",
        "        generated_text = generated_text[1:]\n",
        "        text_seed += next_char\n",
        "    return text_seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "leZCsQl_GRlV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, 60): # train the model for 60 epochs\n",
        "    start_time_inner = time.time()\n",
        "    print('Epoch', epoch, end=' ')\n",
        "    model.fit(x, y, batch_size=128, epochs=1) # fit the model for 1 iteration on the data\n",
        "    start_index = np.random.randint(0, len(data) - max_len - 1) # select a text seed at random\n",
        "    text_seed = data[start_index: start_index + max_len]\n",
        "#     text_seed = 'A Bat who fell upon the ground and was '\n",
        "    print('--- Generating with seed: \"' + text_seed + '\"')\n",
        "    for temperature in [0.2, 0.5, 0.9]:\n",
        "        print('------ temp:', temperature, end='. ')\n",
        "        predicted_text = predict(text_seed, temperature=temperature)\n",
        "        print(predicted_text)\n",
        "    print('Elapsed. This loop:', time.time() - start_time_inner, '. Total:', time.time() - start_time, '\\n')\n",
        "\n",
        "# model = load_model('./model_60.h5') # or load a saved model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYZlot6ZScC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('./model_60.h5')\n",
        "# convert to .bin for browser\n",
        "!curl https://raw.githubusercontent.com/transcranial/keras-js/master/python/encoder.py -o encoder.py\n",
        "!curl https://raw.githubusercontent.com/transcranial/keras-js/master/python/model_pb2.py -o model_pb2.py\n",
        "!python encoder.py -q ./model_60.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRYyQK0QgoPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict('The Hare and the Tortoise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4iJPS-MBiC7h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Credits:**\n",
        "- Deep Learning with Python by [Francois Chollet](https://github.com/fchollet)\n",
        "- http://classics.mit.edu/Aesop/fab.mb.txt\n"
      ]
    }
  ]
}