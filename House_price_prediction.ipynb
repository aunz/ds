{
  "cells": [
    {
      "metadata": {
        "_uuid": "04438d58940611523e8af31c74be376404f9d6b7"
      },
      "cell_type": "markdown",
      "source": "# ðŸ¡ House Price Prediction with Detailed Exploratory Analysis ðŸ˜ï¸\n\nThank you for checking out the kernel ðŸŒ½\n\n\n## Import data"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": false,
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "import numpy as np # stats\nimport pandas as pd # data manipulation\nimport matplotlib.pyplot as plt # graph\nimport seaborn as sns # graph\nfrom warnings import filterwarnings\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint('The train set has {} rows, {} columns'.format(*train.shape))\nprint('The test set has {} rows, {} columns'.format(*test.shape))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce326500eae8101752fe0569c0cbc3f42215f0d6"
      },
      "cell_type": "markdown",
      "source": "Combine both train and test set into one, then drop the 'Id' column"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9c8461b1811c3b9913be628cd5a24cf24750a45"
      },
      "cell_type": "code",
      "source": "all_data = pd.concat([train, test], sort = False, ignore_index = True)\nall_data.drop('Id', axis = 1, inplace = True)\nprint('The all set has {} rows, {} columns'.format(*all_data.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "448bc58656884cd2624a3183135e9431b3ccc3c1"
      },
      "cell_type": "markdown",
      "source": "## Understand the target variable: SalePrice\n- Ranges using the pandas `describe()`\n- Distribution using histogram\n- Normality using histogram, qqplot, and statistics: skewness, shapiro etc"
    },
    {
      "metadata": {
        "_uuid": "548abb0a2dfc0f394db9707293f6ff7ba2802ee1",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from scipy.stats import norm, skew, probplot, shapiro, normaltest, boxcox\nfrom scipy.special import boxcox1p, inv_boxcox1p\n\nsns.set() # sns default theme\nplt.rcParams['figure.figsize'] = [15, 5]\nplt.subplots_adjust(wspace = 0.5)\ndef check_normality(x):\n    m, s = norm.fit(x)\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    sns.distplot(x, fit=norm)\n    plt.legend(['Normal dist\\nÎ¼ = {:.2f}\\nÏƒ = {:.2f}'.format(m, s)], loc='best', )\n    plt.subplot(1, 2, 2)\n    probplot(x, plot = plt)\n    sk = skew(x)\n    sh = shapiro(x)\n    print('''\n    skew: {:.3f}\n    Shapiro-Wilk Test: Statistics = {:.3f}, p = {:.3e}\n    Dâ€™Agostinoâ€™s K^2 Test: Statistics = {:.3f}, p = {:.3e}\n    '''.format(skew(x), *shapiro(x), *normaltest(x)))\n\nprint(train.SalePrice.describe())\ncheck_normality(train.SalePrice)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9501f57a081010a30131240b0af191478c6b540"
      },
      "cell_type": "markdown",
      "source": "Our orignal SalePrice is highly skewed. We can reduce non-normality by:\n- Log transform\n- Boxcox transform\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bb719e3391bdae45a805ea5e91ef8adc3ebdeee"
      },
      "cell_type": "code",
      "source": "check_normality(np.log1p(train.SalePrice))\n\ntmp, maxlog = boxcox(train.SalePrice) # maxlog is the lambda that maximize the log-likelihood function\ncheck_normality(tmp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "225decaf64f7a56c37bb27caf02c2d78231faa1c"
      },
      "cell_type": "markdown",
      "source": "Since boxcox transform produce a lower absolute skew, we will use boxcox transform for subsequence analysis.\n\nWe assgin new variables named **SalePriceBC** (SalePrice boxcox transformed) and **SalePriceL** (SalePrice log transformed) to the all dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "240069ec641a2b6e7f19f99ff103f8e5c1ebafca",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "all_data = all_data.assign(SalePriceBC = np.append(\n    boxcox1p(train.SalePrice, maxlog),\n    np.repeat(np.nan, test.shape[0])\n))\nall_data = all_data.assign(SalePriceL = np.append(\n    np.log1p(train.SalePrice),\n    np.repeat(np.nan, test.shape[0])\n))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86ddabb6f0b42ecb2c03ff8fe92c5dd903db0e6d"
      },
      "cell_type": "markdown",
      "source": "## Understand the explanatory variables (features)\n- They are described in data_description.txt\n- There are 79 of them:\n  - 36 are numeric\n  - 43 are str\n- Of them, 34 contain missing values. Some are only missing in the train set, some are only in the test set, some are in both.\n  \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5388f58a9bbb8b603629db360410c75a4c7ee71b"
      },
      "cell_type": "code",
      "source": "all_data.dtypes.value_counts()\ntmp = pd.DataFrame({\n    'all': all_data.isnull().sum(),\n    'train': train.isnull().sum(),\n    'test': test.isnull().sum(),\n    'dtype': all_data.dtypes,\n}, index = all_data.isnull().sum().index) # need this index otherwise the resulting data.frame orders the index by alphabet\ntmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93ff2d08911b51ea392cd2e1b57656b46e23b9aa"
      },
      "cell_type": "code",
      "source": "tmp.loc[tmp.iloc[:,0:3].any(1).compress(lambda x: x).index.values]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52db3483326e94a34ffaccd95f871af043cd811f"
      },
      "cell_type": "markdown",
      "source": "We will go through all these 34 variables one by one ðŸ¢ and determine how to recover (impute) the missing values. Missing values can be imputed using:\n- Mean\n- Median\n- Most common value\n- Or fancy package such as MICE"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f8c02448c9768709ad3a2b40cc4eb901ffcc61c"
      },
      "cell_type": "code",
      "source": "# a helper function to examine missingness in var with string value\ndef exam(x, hue = 'Dataset', data = all_data.assign(Dataset = all_data.SalePrice.isnull().replace({ False: 'train', True: 'test' }))):\n    data[x].fillna('MISSING', inplace = True)\n    plt.figure()\n    p = sns.countplot(x = x, hue = hue, data = data)\n    for i in p.patches:\n        height = i.get_height()\n        height = 0 if np.isnan(height) else height\n        p.text(i.get_x() + i.get_width() / 2, height + 10, '{:.0f}'.format(height), ha = 'center')\n        \n# also create a new dataframe so not to mutate the \"all\"\nall1 = all_data.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a1309c2ecfd9d1843d46a04c1a26264eb3f9b6b"
      },
      "cell_type": "markdown",
      "source": "### MSZoning\n- 0 missing in the train set\n- 4 missing in the test set\n- Change them to the most common: RL (Residential Low Density)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87b7855171287d52f6b89a57b1ebd76d91edcd9f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "exam('MSZoning')\nall1.MSZoning.fillna('RL', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c6f53c1d074e34144feafb81d32916a10bbb1ef"
      },
      "cell_type": "markdown",
      "source": "### LotFrontage\n- 259 missing in the train set\n- 277 missing in the test set\n- Possible to fill them with mean, median or based on *Neighborhood*\n- Decided to fill them based on median per *Neighborhood* using the entire dataset (train + test). Not sure if this is right?\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ddaa8ed3f491154ebb69eb920a64f9f7072c037"
      },
      "cell_type": "code",
      "source": "all1.LotFrontage.describe()\nall1.LotFrontage = all1.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c0591ceee640c58e4b6b5be9c23d19d1ffcc1e4"
      },
      "cell_type": "markdown",
      "source": "### Alley\n-  2721 missing in the train set\n-  1639 missing in the test set\n- Change them to *None*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08812341257832de5181dd38edd3f293ff5348d1"
      },
      "cell_type": "code",
      "source": "exam('Alley')\nall1.Alley.fillna('None', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e256f35742fc46bf63e403cd111a118c2140c185"
      },
      "cell_type": "markdown",
      "source": "### Utilities\n-  0 missing in train set\n-  2 missing in test set\n\nMost of them are AllPub: All public Utilities (E,G,W,& S). Change the missing to AllPub. The lone 1 observation of NoSeWa is strange! This variable is not likely to help with predictive model, possible to drop it"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eed2c759b9eca29884481055d46828be5842f267"
      },
      "cell_type": "code",
      "source": "exam('Utilities')\nall1.Utilities.fillna('AllPub', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "73455298fd23a3718506156a2e201ee760b45a82"
      },
      "cell_type": "markdown",
      "source": "### Exterior1st & Exterior2nd\n-  0 missing in the train set\n-  1 missing in the test set\n- The exterior material is probably related to RoofStyle and RoofMatl. The missing observation has RoofStyle of Flat and RoofMatl of Tar&Grv. Houses with these roofs have exterior as Plywood. So change the missing value to *Plywood*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9a3dcb205a623ab60df0966ea60e358cea7929a"
      },
      "cell_type": "code",
      "source": "exam('Exterior1st')\nexam('Exterior2nd')\n\nall1[all1.Exterior1st.isnull()]\nall1[all1.RoofStyle == 'Flat'].Exterior1st.value_counts()\nall1[all1.RoofStyle == 'Flat'].Exterior2nd.value_counts()\nall1[all1.RoofMatl == 'Tar&Grv'].Exterior1st.value_counts()\nall1[all1.RoofMatl == 'Tar&Grv'].Exterior2nd.value_counts()\n\nall1.Exterior1st.fillna('Plywood', inplace = True)\nall1.Exterior2nd.fillna('Plywood', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "080b761687747b741e7d8dfb2059e97e9ea82f65"
      },
      "cell_type": "markdown",
      "source": "### MasVnrType & MasVnrArea\n-  8 missing in the train set\n-  16 missing in the test set\n- They could be related to LotShape, Neighborhood, Neighborhood. But as the most common is None, so change them all to *None* for MasVnrType, and to *0* for MasVnrArea"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f0fca6ec84be3f477b09f66a4ccb789b23a809f"
      },
      "cell_type": "code",
      "source": "exam('MasVnrType')\nall1.MasVnrType.fillna('None', inplace = True)\nall1.MasVnrArea.fillna(0, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e1481e3d3d84f577056d23223428822336e0f54"
      },
      "cell_type": "markdown",
      "source": "### Basement: BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath \n- 37+ missing in train set\n- 42+ missing in test set\n- The decription indicates NA as No Basement\n- A careful examination shows that 9 houses actually have a basement, but unfinished or being restored.\n- Examine these 9 one by one and change them accordingly\n- Change the rest to *None*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9ab5113631c4ef0a83070008215ebe5fd1cefbd"
      },
      "cell_type": "code",
      "source": "exam('BsmtQual')\nexam('BsmtCond')\nexam('BsmtExposure')\nexam('BsmtFinType1')\nexam('BsmtFinType2')\n\n# these houses below actually have basement\nall1[(\n    all1.BsmtQual.isnull() |\n    all1.BsmtCond.isnull() |\n    all1.BsmtExposure.isnull() | \n    all1.BsmtFinType1.isnull() |\n    all1.BsmtFinType2.isnull()\n) & all1.TotalBsmtSF > 0]\n\n# change these houses accordingly\nall1.loc[all1.BsmtQual.isnull() & all1.TotalBsmtSF > 0, 'BsmtQual'] = 'TA' # TA is the most common\nall1.loc[all1.BsmtCond.isnull() & all1.TotalBsmtSF > 0, 'BsmtCond'] = 'TA'\nall1.loc[all1.BsmtExposure.isnull() & all1.TotalBsmtSF > 0, 'BsmtExposure'] = 'No' # No is the most common\nall1.loc[all1.BsmtFinType2.isnull() & all1.TotalBsmtSF > 0, 'BsmtFinType2'] = 'Unf' # Unf is the most common\n\n# the rest filled with None\nall1.BsmtQual.fillna('None', inplace = True)\nall1.BsmtCond.fillna('None', inplace = True)\nall1.BsmtExposure.fillna('None', inplace = True)\nall1.BsmtFinType1.fillna('None', inplace = True)\nall1.BsmtFinType2.fillna('None', inplace = True)\n\n# or 0 for numeric\nall1.BsmtFinSF1.fillna(0, inplace = True)\nall1.BsmtFinSF2.fillna(0, inplace = True)\nall1.BsmtUnfSF.fillna(0, inplace = True)\nall1.TotalBsmtSF.fillna(0, inplace = True)\nall1.BsmtFullBath.fillna(0, inplace = True)\nall1.BsmtHalfBath.fillna(0, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f4203940e857205406692a46bcbab9a9ae6c6406"
      },
      "cell_type": "markdown",
      "source": "### Electrical\n-  1 missing in the train set\n-  0 missing in the test set\n- Change them to *SBrkr* (the most common)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8443907f0bed25fc7486ffbc9aa5d5274c15084"
      },
      "cell_type": "code",
      "source": "exam('Electrical')\nall1.Electrical.fillna('SBrkr', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06ec8c63288132b065e7f142494a4dc45a6198ca"
      },
      "cell_type": "markdown",
      "source": "### KitchenQual\n-  0 missing in the train set\n-  1 missing in the test set\n- Change them to *TA* (the most common)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c764aa537bc545722cf2369b4cf5b2c6971cd33"
      },
      "cell_type": "code",
      "source": "exam('KitchenQual')\nall1.KitchenQual.fillna('TA', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "982973ffa29798250903bdb79ab65b74035d4f5a"
      },
      "cell_type": "markdown",
      "source": "### Functional\n-  0 missing in the train set\n-  2 missing in the test set\n- Change them to *Typ* (the most common)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "816ba2d5e74ad9de4e20c197516e1e45fe04c319"
      },
      "cell_type": "code",
      "source": "exam('Functional')\nall1.Functional.fillna('Typ', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "873cbe927387307c31757bb181ed38fd7bc8d57e"
      },
      "cell_type": "markdown",
      "source": "### FireplaceQu\n- 690 missing in the train set\n- 730 missing in the test set\n- Change them to *None*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50794fb5c511719adb15f4545e878dedb17e1eef",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "exam('FireplaceQu')\nall1.FireplaceQu.fillna('None', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f4e3d493e24330cbd263689fccb5001e65f79f9"
      },
      "cell_type": "markdown",
      "source": "### Garage: GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond\n- ~81 missing in the train set\n-  ~78 missing in the test set\n- There is an observation where GarageYrBlt is 2207, must be a typo, will change it to 2007, as YearBuilt in 2006\n- There are also 18 observations where GarageYrBlt < YearBuilt. Set GarageYrBlt = YearBuilt\n- A careful examination found that in the test set, 2 observation actually have a GarageType (Detchd) but are missing in the other columns .\n- Change these 2 accordingly.\n- For the rest, change GarageType, GarageFinish, GarageQual and GarageCond to *None*, GarageCars and GarageArea to *0*, GarageYrBlt to be the same as median of GarageYrBlt of houes with the same *YearBuilt*, if no median, fill GarageYrBlt with YearBuilt\n- The first GarageYrBlt was 1895 (Detchd) in observation 2217 (the first true automobile was thought to be invented in 1885), interesting."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6268431949ab9bae036b53c9cacc4941b7631b6"
      },
      "cell_type": "code",
      "source": "all1.GarageYrBlt.describe()\n\nall1.GarageYrBlt.sort_values(ascending = False).index[0] # observation: 2592\nall1.loc[2592, ['GarageYrBlt', 'YearBuilt', 'YearRemodAdd', 'YrSold']]\n\n# change to 2007\nall1.loc[2592, 'GarageYrBlt'] = 2007\nall1.loc[2592, ['GarageYrBlt', 'YearBuilt', 'YearRemodAdd', 'YrSold']]\n\nsns.scatterplot(x = 'YearBuilt', y = 'GarageYrBlt', data = all1)\ntmp = all1.GarageYrBlt < all1.YearBuilt # those where a garage was built before the house\nall1.loc[tmp, ['GarageYrBlt', 'YearBuilt']]\nall1.loc[tmp, 'GarageYrBlt'] = all1.loc[tmp, 'YearBuilt']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62aab347fb4fd772ee8845f77a7858372f6f5853"
      },
      "cell_type": "code",
      "source": "# all1[all1.GarageType != 'None'].GarageYrBlt.sort_values()\n# all1.loc[2217]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5931c751e135b9a1fe72179f9be1b42eb53dab85"
      },
      "cell_type": "code",
      "source": "exam('GarageType')\nexam('GarageFinish')\nexam('GarageQual')\nexam('GarageCond')\n\n# 2 observations with a garage\nall1.loc[all1.GarageType.notnull() & all1.GarageCond.isnull(), 'GarageYrBlt'] = all1.loc[all1.GarageType == 'Detchd', 'GarageYrBlt'].median() # 1962, the median of Detchd type\nall1.loc[all1.GarageType.notnull() & all1.GarageCond.isnull(), 'GarageFinish'] = 'Unf' # Unf is the most common\nall1.loc[all1.GarageType.notnull() & all1.GarageCond.isnull(), 'GarageQual'] = 'TA' # TA is the most common\nall1.loc[all1.GarageType.notnull() & all1.GarageCond.isnull(), 'GarageCond'] = 'Unf' # TA is the most common\nall1.loc[all1.GarageType.notnull() & all1.GarageCars.isnull(), 'GarageCars'] = all1.loc[all1.GarageType == 'Detchd', 'GarageCars'].median()\nall1.loc[all1.GarageType.notnull() & all1.GarageArea.isnull(), 'GarageArea'] = all1.loc[all1.GarageType == 'Detchd', 'GarageArea'].median()\n\n# the rest\nall1.GarageType.fillna('None', inplace = True)\nall1.GarageCars.fillna(0, inplace = True)\nall1.GarageArea.fillna(0, inplace = True)\nall1.GarageFinish.fillna('None', inplace = True)\nall1.GarageQual.fillna('None', inplace = True)\nall1.GarageCond.fillna('None', inplace = True)\n\nall1.GarageYrBlt = all1.groupby('YearBuilt').GarageYrBlt.transform(lambda x: x.fillna(x.median()))\ntmp = all1.GarageYrBlt.isnull()\nall1.loc[tmp, 'GarageYrBlt'] = all1.loc[tmp, 'YearBuilt']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "efa70a94a4506a36544b1c7e8e0025af5fff23fb"
      },
      "cell_type": "markdown",
      "source": "### PoolQC \n- 1453 missing in the train set\n- 1456 missing in the test set\n- Change them all to *None*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e82121a3159aa23b0ff1bf0eb402540339036ab"
      },
      "cell_type": "code",
      "source": "exam('PoolQC')\nall1.PoolQC.fillna('None', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a086973a73bff0e3e84825a2e38b5b79f8e632ce"
      },
      "cell_type": "markdown",
      "source": "### Fence\n- 1179 missing in the train set\n- 1169 missing in the test set\n- Change them all to *None*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c142b61c4cb3bf34e86a5be975ae0a102415bd9"
      },
      "cell_type": "code",
      "source": "exam('Fence')\nall1.Fence.fillna('None', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8906f1969d30ad305d4b3e4005f1a887f07d9173"
      },
      "cell_type": "markdown",
      "source": "### MiscFeature \n- 1406 missing in the train set\n- 1408 missing in the test set\n- Change them all to *None*\n- Most of the houses have no additional feature, Shed is probably just Gar2, 1 house has a tennis court ðŸŽ¾, fancy âœ¨"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5edb3593d8401ad532e1e5b4a75482dc3018d33e"
      },
      "cell_type": "code",
      "source": "exam('MiscFeature')\nall1.MiscFeature.fillna('None', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "220ce81a9ffca141f988ba82c9bd9a035b29c8bf"
      },
      "cell_type": "markdown",
      "source": "### SaleType \n- 0 missing in the train set\n- 1 missing in the test set\n- Change them *WD*\n- There is one observation in the test set that the house was sold before it was built, YearBuilt > YrSold"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ff2ed0eeef1c79daa5d8d0728e30f345ce57017"
      },
      "cell_type": "code",
      "source": "exam('SaleType')\nall1.SaleType.fillna('WD', inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a647f9ccc6e0d8a48346c6b5624c7fa7e2b00c4f"
      },
      "cell_type": "code",
      "source": "exam('SaleCondition')\nall1[all1.YearBuilt > all1.YrSold] # this house SaleCondition was Partial",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f5c4d3fd2a3d07c99cce2b65e8c6f0504d8641dd"
      },
      "cell_type": "markdown",
      "source": "### Now there should not be any missing in the train and test dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "95c52226dfd18d543de2dcddcaf8e0f1b0d13629"
      },
      "cell_type": "code",
      "source": "all1.loc[:, ~all1.columns.isin(['SalePrice', 'SalePriceBC'])].isnull().any().any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59467a94f89b159b8eac3a97c1caceee8ad45f4a"
      },
      "cell_type": "markdown",
      "source": "## Understand the relationship between the target variable (SalePrice) and explanatory variables\n- Compute correlation matrix\n- Visualise using a heatmap\n- The most correlated variables to SalePrice are:\n  - OverallQual\n  - GrLivArea\n  - GarageCars\n  - GarageArea\n  - TotalBsmtSF\nThese make sense as the bigger the house the more expensive it is\n  \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fada8109d7efdd5d17e15c673c54feb53bd5bdf2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "tmp = all1[all1.SalePrice.notnull()]\ntmp_corr = tmp.corr()\n\nplt.figure(figsize = (15, 9))\nsns.heatmap(tmp_corr, square = True)\n\ntmp_corr.SalePrice.sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03cc07509365e3dd5abc3ae35f32c1d2a01ff981"
      },
      "cell_type": "markdown",
      "source": "### Outliers in GrLivArea\n- There are two observations with a large living area (>4000) but were sold relatively cheaply (<200,000). They are true outliers (also according to http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt)\n- It's not ideal to remove data, but in this case, it's justfied because we are interested in predicting prices for **typical** houses.\n- I will create a new variable named **IsOutlier** and then later on we can compare models where outliers are removed vs models where outliers are not removed."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6c28636481fa92fc2d6d82e6a70cab052338303",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "plt.figure(figsize = (12, 9))\nsns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = all1)\n\nall1 = all1.assign(IsOutlier = np.where((all1['GrLivArea'] > 4000) & (all1['SalePrice'] < 200000), 1, 0))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "3866d6e8df91e6030bdd2daedde11965096d6db9"
      },
      "cell_type": "code",
      "source": "# Also check other variables\n\nsns.scatterplot(x = 'LotArea', y = 'SalePrice', data = all1); plt.figure()\nsns.scatterplot(x = 'MasVnrArea', y = 'SalePrice', data = all1); plt.figure()\nsns.scatterplot(x = 'TotalBsmtSF', y = 'SalePrice', data = all1); plt.figure()\nsns.scatterplot(x = 'GarageArea', y = 'SalePrice', data = all1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0106c44e3365c82df0a6b3dd312a1925a9cc5597"
      },
      "cell_type": "markdown",
      "source": "### OverallQual\n- This feature has the highest correlation with SalePrice\n- It's worth having a more detailed look\n- The higher the quality, the higher the sale price (as expected)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "523e10a076d997a6f34ae0591aee7bc428dcc83b"
      },
      "cell_type": "code",
      "source": "sns.boxplot(x = 'OverallQual', y = 'SalePrice', color = 'seagreen', data = all1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02bc2fa878451caba4ec92a1f0abf84c4d75cd56"
      },
      "cell_type": "markdown",
      "source": "## dtypes in predictors\nSome of the predictors should be string:\n- MSSubClass\n- MoSold, at the moment it's from 1 to 12. It shouldn't make sense that Dec is \"greater\" than Jan\n\nSome string predictors should be numeric:\n- OverallQual &  OverallCond are numeric while ExterQual &  ExterCond, BsmtQual & BsmtCond, HeatingQC, KitchenQual, FireplaceQu, GarageQual & GarageCond, PoolQC are string\n- Should \\*Qual and \\*Cond be converted to numeric, if so, how? One-hot, get_dummies, binary or https://github.com/scikit-learn-contrib/categorical-encoding?\n\n### Qual & Cond\nQual and Cond are ranked from Po Poor to Ex Excellent, so it makes senses to convert them to numeric from 1 to 5"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a06d74b3ea9d8459b3e8ca68c8f04d7de102f662"
      },
      "cell_type": "code",
      "source": "all1.MSSubClass = all1.MSSubClass.astype(str)\nall1.MoSold = all1.MoSold.astype(str)\n\ncolumns = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\nvalues = { 'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5 }\nall1[columns] = all1[columns].replace(values)\n\ndef exam2(x):\n    plt.figure()\n    sns.boxplot(x = x, y = 'SalePrice', data = all1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5436394e4ca06bdcfee90553ab7b25df1e023324"
      },
      "cell_type": "markdown",
      "source": "### Street, Alley, Land, style\n- Street and Alley seem ordinal, so they will be converted to numeric\n- LotShape, LandContour, LotConfig, LandSlope do not seem ordinal, so leave them as they are\n- Condition1 & Condition2 are not ordinal\n- BldgType is not ordinal\n- HouseStyle is marginally ordinal, but will leave it as it (replated to GrLivArea)\n- RoofStyle, RoofMatl, MasVnrType, Foundation are not ordinal\n- BsmtExposure looks ordinal\n- Heating is not ordinal\n- CentralAir can be made ordinal\n- Electrical is not ordinal\n- Functional can be made ordinal\n- GarageType is not ordinal\n- GarageFinish can be made ordinal\n- PavedDrive can be made ordinal\n- MiscFeature, SaleType, SaleCondition are not ordinal"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "556ec1b7e4d1ba58e3e36a4f39cd4acfd3deb3a8",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "exam2('Street'); all1.Street.replace({ 'Grvl': 0, 'Pave': 1 }, inplace = True)\nexam2('Alley'); all1.Alley.replace({ 'No': 0, 'Grvl': 1, 'Pave': 2 }, inplace = True)\n\n# these don't seem ordinal\n# exam2('LotShape'); exam2('LandContour'); exam2('LotConfig'); exam2('LandSlope')\n# exam2('Condition1'); exam2('Condition2')\n# exam2('BldgType'); exam2('HouseStyle')\n# exam2('RoofStyle'); exam2('RoofMatl'); exam2('MasVnrType'); exam2('Foundation')\n# exam2('Heating')\n\nexam2('BsmtExposure'); all1.BsmtExposure.replace({ 'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4 }, inplace = True)\nvalues = { 'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6 }\nexam2('BsmtFinType1'); all1.BsmtFinType1.replace(values, inplace = True)\nexam2('BsmtFinType2'); all1.BsmtFinType2.replace(values, inplace = True)\n\n# exam2('Heating') # not ordinal\nexam2('CentralAir'); all1.CentralAir.replace({ 'Y': 1, 'N': 0 }, inplace = True)\n\n# exam2('Electrical') # not ordinal\n\nexam2('Functional'); all1.Functional.replace({ 'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7 }, inplace = True)\n\n# exam2('GarageType') # not ordinal\nexam2('GarageFinish'); all1.GarageFinish.replace({ 'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3 }, inplace = True)\nexam2('PavedDrive'); all1.PavedDrive.replace({ 'N': 0, 'P': 1, 'Y': 2 }, inplace = True)\n\n# exam2('MiscFeature'); exam2('SaleType'); exam2('SaleCondition')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c16f802a643bfb8b9c5aa5dd2268e3c99a7df942"
      },
      "cell_type": "markdown",
      "source": "## Deriving new variables/features (feature engineering)\n### Area\n- We have: LotArea, MasVnrArea, GrLivArea, TotalBsmtSF, GarageArea, PoolArea, 1stFlrSF, 2ndFlrSF, LowQualFinSF, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch\n- Add a TotalArea = GrLivArea + TotalBsmtSF + GarageArea + MasVnrArea \n- Now, correlation between TotalArea & SalePrice is 0.8598, much higher than individual areas alone.\n- I have tried adding Porches and/or Deck into TotalArea, but the resulting correlations decreased, so I did not add them to the TotalArea\n- Note: adding areas together to derive TotalArea will cause some multi-collinearity. Later in the model buildings, we may need to drop some variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b021ee158f5a78fe5cef1e3d1da8a5872315aa46"
      },
      "cell_type": "code",
      "source": "# GrLivArea is usually the sum of 1stFlrSF and 2ndFlrSF, except a few \ntmp = all1[['1stFlrSF', '2ndFlrSF']].apply(sum, axis = 1) == all1.GrLivArea\nall1.loc[tmp == 0, ['GrLivArea', '1stFlrSF', '2ndFlrSF']]\n\n# Assign TotalArea\nall1 = all1.assign(TotalArea = all1[['GrLivArea', 'TotalBsmtSF', 'GarageArea', 'MasVnrArea']].apply(sum, axis = 1))\nall1[['TotalArea', 'GrLivArea', 'TotalBsmtSF', 'GarageArea', 'MasVnrArea', 'SalePrice']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8f05917892f8650d64b15c53a3af48237cb0bc7"
      },
      "cell_type": "markdown",
      "source": "### Bathroom\n- We have: BsmtFullBath, BsmtHalfBath, FullBath, HalfBath\n- Add a Bath = BsmtFullBath  + BsmtHalfBath / 2+ FullBath + HalfBath / 2\n- Now correlation between Bath & SalePrice is 0.6359"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5b4d08e20299e8a510f310fe7cd272afd107e85"
      },
      "cell_type": "code",
      "source": "all1 = all1.assign(Bath = all1.BsmtFullBath + all1.BsmtHalfBath / 2 + all1.FullBath + all1.HalfBath / 2)\nall1[['Bath', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'SalePrice']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "486c20cf55d49adb03ef242e8a5436a35fe7a12b"
      },
      "cell_type": "markdown",
      "source": "### House age & remodelling\n- YearBuilt, YearRemodAdd, YrSold\n- From these variables, we can determine:\n - The age of the house: YrSold - YearBuilt\n - Has the house ever been remodelled: YearBuilt != YearRemodAdd\n - ~~Was the house new when sold: YrSold == YearBuilt~~\n- We see that:\n - There is a slight downward trend for SalePrice the older the house\n - Remodelled houses were slightly of lower prices (probably also remodelled houses were older, smaller?)\n - New houses were much higher in prices"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59c08c4f8b9e183029a26f986d76d8f7cf8ac23b"
      },
      "cell_type": "code",
      "source": "all1 = all1.assign(Age = all1.YrSold - all1.YearBuilt)\nall1.loc[all1.Age < 0, 'Age'] = 0 # There was one house where YrSold < YearBuilt\n\nall1 = all1.assign(IsRemodelled = np.where(all1.YearBuilt == all1.YearRemodAdd, 0, 1))\n\n# all1 = all1.assign(IsNew = np.where(all1.YrSold <= all1.YearBuilt, 1, 0)) # use <= because there was one house where YrSold < YearBuilt\n# all1.loc[all1.SaleType == 'New', 'IsNew'] = 1 # some where \n\nall1.Age.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "b9248c97f067fe7162c08ea7b2157dd175c55989"
      },
      "cell_type": "code",
      "source": "sns.distplot(all1.Age, bins = 50); plt.figure()\nsns.scatterplot(x = 'Age', y = 'SalePrice', data = all1); plt.figure()\nsns.boxplot(x = 'IsRemodelled', y = 'SalePrice', data = all1); plt.figure()\nsns.boxplot(x = 'SaleType', y = 'SalePrice', data = all1); plt.figure()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1871f79ffd02f4fb6e8258dcc730ac0da9f969f1"
      },
      "cell_type": "markdown",
      "source": "## Skewness in numeric predictors\n- We now have 55 numeric variables,  33 of them have skewness more than 0.75 \n- We can reduce skewness using log1p or boxcox for some of them\n- It appears that boxcox reduce skewness more than log1p"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "22878abe3d321fb684677790c3b16c9e3524f6ec"
      },
      "cell_type": "code",
      "source": "filterwarnings('ignore')\ntmp = all1.skew().compress(lambda x: abs(x) > 0.75).sort_values()\ntmp = tmp[tmp.index != 'MSSubClass']\ntmp = pd.DataFrame({\n    'ori': tmp,\n    'after with log': [skew(np.log1p(all1[x]), nan_policy = 'omit') for x in tmp.index],\n    'after with boxcox': [skew(boxcox(all1[x] + 1)[0], nan_policy = 'omit') for x in tmp.index]\n})\nfilterwarnings('ignore')\ntmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af148f54b94d3f909650f93b8faf2af784439937"
      },
      "cell_type": "code",
      "source": "for v in ['Functional', 'BsmtCond', 'GarageQual', 'PavedDrive', 'BsmtQual', 'TotRmsAbvGrd', 'ExterQual', '2ndFlrSF', 'BsmtUnfSF', 'BsmtExposure', 'TotalBsmtSF', 'ExterCond', 'BsmtFinSF1', 'TotalArea', 'LotFrontage', 'WoodDeckSF', 'OpenPorchSF', 'MasVnrArea', 'BsmtFinType2', 'ScreenPorch', 'EnclosedPorch', 'BsmtFinSF2', 'KitchenAbvGr', '3SsnPorch', 'LowQualFinSF', 'PoolArea', 'PoolQC', 'MiscVal']:\n    all1[v] = boxcox(all1[v] + 1)[0]\n\nfor v in ['GrLivArea', '1stFlrSF', 'LotArea']:\n    all1[v] = np.log1p(all1[v])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "537b569f2301e46e781bf3d3723184c9b2f93474"
      },
      "cell_type": "code",
      "source": "all1.skew().compress(lambda x: abs(x) > 0.75).sort_values()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e375618adf2ae5ccaf0b82dfbcdaf10e032dd3e4"
      },
      "cell_type": "markdown",
      "source": "## Label encoding\n\n- We are now left with 28 variables of type `str`\n- We can convert them using LabelEncoder, OneHotEncoder, LabelBinarizer or MultiLabelBinarizer\n- In this instance, I am going to use the OneHotEncoder\n- The resulting data.frame is now all numeric, containing 301 columns"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d33cf3bc9347054e2d87b3be0201a8401ff16b3"
      },
      "cell_type": "code",
      "source": "all1.dtypes.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "203a4ce0925a21e6362c5d39ec32dc0520a01dfa",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from category_encoders import OneHotEncoder, BinaryEncoder\n# all1_n = BinaryEncoder().fit_transform(all1) # tried this, but cv is lower than onehot\nall1_n = pd.get_dummies(all1) # similar to all1_n = OneHotEncoder().fit_transform(all1)\n\nprint(all1_n.shape)\nprint(all1_n.dtypes.value_counts())\n\ntrain_index = all1_n.SalePrice.notnull()\n\n# remove the outliers\noutliers = all1_n.IsOutlier == 1\nX = all1_n.loc[~outliers & train_index, ~all1_n.columns.isin(['SalePrice', 'SalePriceBC', 'SalePriceL', 'IsOutlier'])]\ny = all1_n.loc[~outliers & train_index, 'SalePriceBC'] # boxcox transformed SalePrice\nyL = all1_n.loc[~outliers & train_index, 'SalePriceL'] # log transformed SalePrice\nyO = all1_n.loc[~outliers & train_index, 'SalePrice'] # Original SalePrice\n\nX_test = all1_n.loc[~train_index, ~all1_n.columns.isin(['SalePrice', 'SalePriceBC', 'SalePriceL', 'IsOutlier'])]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65e3b0bc6087b98e5c30d505179a2a9cfc1e5f90"
      },
      "cell_type": "markdown",
      "source": "## Modelling\n\n### Libraries"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92549142a263a79f23f5bc9ad2ee3e431127d8fd"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n# score model performance\ncv = KFold(n_splits = 5, shuffle = True, random_state = 0)\ndef score(model, X = X, y = y, des = ''):\n    rmse = np.sqrt(-cross_val_score(model, X = X, y = y, scoring = 'neg_mean_squared_error', cv = cv))\n    print(des, '\\n', *rmse, '\\nMean:', rmse.mean(), '\\nSD:', rmse.std())\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b18b9212c38567a59ccfb20fffb390f0674028f"
      },
      "cell_type": "markdown",
      "source": "### Baseline\n\nIf we guess all the SalePrice as the mean, the RMSE are:\n- 0.1583 (using boxcox transformed y)\n- 0.3996 (using log transformed y)\n- 79467.79 (using original y)\nOur models need to beat them\n\n### Good old linear regression\n- The mean RMSE is 0.1568, not much better than baseline"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49c66a23b4e3850941af463ac228cc65135bea54"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, LassoLarsIC, ElasticNet, ElasticNetCV, BayesianRidge",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92988abc2ea5e39198b0cd8e00e13c70c9b1cd94"
      },
      "cell_type": "code",
      "source": "# baseline, if we guess all y as y.mean()\n# np.sqrt(mean_squared_error(y, np.repeat(y.mean(), len(y)))) # boxcox\n# np.sqrt(mean_squared_error(yL, np.repeat(yL.mean(), len(yO)))) # log\n# np.sqrt(mean_squared_error(yO, np.repeat(yO.mean(), len(yL)))) # original",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2fd445b3f7ed3ff922aaaf4d31c17d8fd3442f4"
      },
      "cell_type": "code",
      "source": "model = make_pipeline(RobustScaler(), LinearRegression())\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');\n# score(model, y = yO, des = '\\nUsing original SalePrice as y');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f0c3d09f600900d40aeb641a0474fb451d613562"
      },
      "cell_type": "markdown",
      "source": "### Ridge\nThis improved significantly without any optimisation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8501a72a95781b06cd41883607447c0f8ac6c701"
      },
      "cell_type": "code",
      "source": "filterwarnings('ignore')\n\nmodel = make_pipeline(RobustScaler(), Ridge())\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');\n# score(model, y = yO, des = '\\nUsing original SalePrice as y');\n\nfilterwarnings('default')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e99e44f331669c3ccefd1271769d4d036ead1e7e"
      },
      "cell_type": "code",
      "source": "filterwarnings('ignore')\n\nscore(make_pipeline(Ridge()), des = 'No scaling'); # without scaling seems a little better\nscore(make_pipeline(StandardScaler(), Ridge()), des = '\\nUsing StandardScaler'); # standard scaler seems a litte bit worse\n\nfilterwarnings('default')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d8e47bee305c6c8b29658b967b59da9a004cfe0"
      },
      "cell_type": "markdown",
      "source": "### Lasso\n- Find the best alpha using LassoCV\n- RMSE is higher when NOT excluding the outliers\n- Lasso also perferoms feature selection. From the beginning 168 features of X, it redues to 83.\n- The highest positive coef are:\n  - GrLivArea\n  - OverallQual\n  - Neighborhood\n- The highest negative coef are:\n  - MSZoning\n  - Age\n  - BsmtFinType"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25e6d8f654c49a1739b8026db3a23c3d3ab6228"
      },
      "cell_type": "code",
      "source": "# Using LassoCV to find the best alpha\nfilterwarnings('ignore')\nmodel = LassoCV(\n    alphas = np.logspace(-5, 0.5, 50),\n    random_state = 0,\n    cv = cv,\n    max_iter = 100000,\n    n_jobs = -1\n)\nmodel.fit(RobustScaler().fit_transform(X), y)\nfilterwarnings('default')\n\nalpha = model.alpha_\n\ndef plot_rmse(alphas, rmse):    \n    tmp = pd.DataFrame({\n        'alpha': alphas,\n        '-log(alpha)': -np.log(alphas),\n        'rmse': rmse,\n    })\n    sns.scatterplot(x = '-log(alpha)', y = 'rmse', data = tmp)\n    tmp = tmp[tmp.rmse == min(tmp.rmse)] # the min\n    plt.annotate(\n        'alpha: {:.6f}\\nscore: {:.6f}'.format(*tmp[['alpha', 'rmse']].iloc[0]),\n        xy = (tmp['-log(alpha)'], tmp['rmse'] + 0.001),\n        xytext = (7.5, tmp['rmse'] + 0.025),\n        arrowprops = dict(arrowstyle = '->', color = 'green')\n    )\n\nplot_rmse(model.alphas_, np.sqrt(np.mean(model.mse_path_, 1)))\ntmp = pd.DataFrame({\n    'coef': model.coef_,\n    'var': X.columns.values\n})\ntmp = tmp.drop(tmp[abs(tmp.coef) < 0.01].index).sort_values('coef')\ntmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d4a2fb135881e77eccd19f97efbae5d0369a888"
      },
      "cell_type": "code",
      "source": "# Using GridSearchCV to find the best alpha\nfrom sklearn.model_selection import GridSearchCV\n\nfilterwarnings('ignore')\nmodel = GridSearchCV(\n    Lasso(random_state = 0),\n    param_grid = { 'alpha': np.logspace(-5, 0, 50) },\n    scoring = 'neg_mean_squared_error',\n    n_jobs = -1,\n    return_train_score = True,\n    cv = cv,\n)\nmodel.fit(RobustScaler().fit_transform(X), y)\nfilterwarnings('default')\n\nfor i in model.cv_results_: print(i)\nplot_rmse(\n    model.cv_results_['param_alpha'].data.astype(np.float64),\n    np.sqrt(-model.cv_results_['mean_test_score'])\n)\n\nalpha = model.best_params_['alpha']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c058dabe42acbc1cfcf63d7cd8b545982545baca"
      },
      "cell_type": "code",
      "source": "model = make_pipeline(RobustScaler(), Lasso(alpha = 0.000212, random_state = 0, max_iter = 10000))\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');\n# score(model, y = yO, des = '\\nUsing original SalePrice as y'); # doesn't converge",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a366784a961702d397e5d67575a730fcebc0af0a",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# LassoLarsIC\nfilterwarnings('ignore')\nmodel = make_pipeline(RobustScaler(), LassoLarsIC())\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');\n# score(model, y = yO, des = '\\nUsing original SalePrice as y');\nfilterwarnings('default')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5398894cbfb1944eee8b77a8c11a8c866c31cb66"
      },
      "cell_type": "code",
      "source": "# test the effects of NOT excluding the outliers\nmodel = make_pipeline(RobustScaler(), Lasso(alpha = 0.000212, random_state = 0, max_iter = 100000))\nscore(\n    model,\n    X = all1_n.loc[train_index, ~all1_n.columns.isin(['SalePrice', 'SalePriceBC', 'SalePriceL', 'IsOutlier'])],\n    y = all1_n.loc[train_index, 'SalePriceBC']\n);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd54d16c6358fb6108d271494734327f82f38956"
      },
      "cell_type": "markdown",
      "source": "### Elastic Net\n- Use ElasticNetCV to find the best alpha and l1_ratio\n- Similar to Lasso, it selected similar vars: GrLivArea, OverallQual, SaleCondition, MSZoning, Age, SaleType"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60a48f5b1e296631060a48e2f1c248fa42ae575d"
      },
      "cell_type": "code",
      "source": "# # Using ElasticNetCV to find the best alpha and l1_ratio\nmodel = ElasticNetCV(\n    random_state = 0, cv = cv, max_iter = 100000, n_jobs = -1,\n    alphas = np.logspace(-4, 0, 25),\n    l1_ratio = [0.1, .5, .7, .9, .95, .99]\n)\nfilterwarnings('ignore')\nmodel.fit(X, y)\nfilterwarnings('default')\n\nprint('Best alpha: {}\\nbest l1_ratio: {}'.format(model.alpha_, model.l1_ratio_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16e147cb8415fadacf6ec55ea4ec287b66919444",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "model = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.001695, l1_ratio = 0.1, random_state = 0, max_iter = 100000))\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2e19a3bbfd483ee15f552934b9aca27d22030c4"
      },
      "cell_type": "code",
      "source": "pd.DataFrame({\n    'Id': test.Id,\n    'SalePrice': inv_boxcox1p(model.fit(X, y).predict(X_test), maxlog)\n}).to_csv('submission_elastic_net_y.csv', index = False)\n\npd.DataFrame({\n    'Id': test.Id,\n    'SalePrice': np.expm1(model.fit(X, yL).predict(X_test))\n}).to_csv('submission_elastic_net_yL.csv', index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2903872c7907cde9c642aa1231b095800fe6fe3c"
      },
      "cell_type": "code",
      "source": "# BayesianRidge\nmodel = make_pipeline(RobustScaler(), BayesianRidge())\nscore(model, des = 'Using boxcox transformed SalePrice as y');\nscore(model, y = yL, des = '\\nUsing log transformed SalePrice as y');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de7a3c4d5cb07be9f87d93e6c48e4e51246538bc"
      },
      "cell_type": "markdown",
      "source": "### Random Forest\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc73db1a4927de9223b09b2747022b2e2a960a26"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33ad3ca57b52b248032043e5325d7db6842ab597"
      },
      "cell_type": "code",
      "source": "model = make_pipeline(RandomForestRegressor(n_estimators = 500, n_jobs = -1))\nfilterwarnings('ignore')\nscore(model);\nfilterwarnings('default')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60611700bc753e0a196806294649f41b70fec6ae"
      },
      "cell_type": "markdown",
      "source": "### Gradient Boosting"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8be413c29db24323262a590e5423b077c7f3ea7"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingRegressor\nmodel = make_pipeline(GradientBoostingRegressor())\nfilterwarnings('ignore')\nscore(model);\nscore(model, y = yL);\nfilterwarnings('default')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19424e27c18b394505ac1fd3bd499da64e379fda"
      },
      "cell_type": "markdown",
      "source": "### XGBoost "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a19c5ddc9d7cc32194759cd8c2d4035068bedad"
      },
      "cell_type": "code",
      "source": "# from xgboost import XGBRegressor\nfrom xgboost.sklearn import XGBRegressor # with scikit-learn wrapper\nmodel = XGBRegressor(\n    learning_rate = 0.046777,\n    n_estimators = 250,\n    max_depth = 4,\n    min_child_weight = 1,\n    gamma = 0,\n    subsample = 0.5,\n    colsample_bytree = 0.6,\n    reg_alpha = 0.17704,\n    reg_lambda = 0.75,\n    n_jobs = -1,\n)\nscore(model);\nscore(model, y = yL);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c911971d12059edcad8bc57bcfbca1b3ffe0ccf6"
      },
      "cell_type": "code",
      "source": "pd.DataFrame({\n    'Id': test.Id,\n    'SalePrice': inv_boxcox1p(model.fit(X, y).predict(X_test), maxlog)\n}).to_csv('submission_xgb_y.csv', index = False)\n\npd.DataFrame({\n    'Id': test.Id,\n    'SalePrice': np.expm1(model.fit(X, yL).predict(X_test))\n}).to_csv('submission_xgb_yL.csv', index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b3b095f6eece9ce62d728c2f8592aa0c308651e"
      },
      "cell_type": "code",
      "source": "# tuning XGBRegressor\n# # 1, tune n_estimators, best value found: 250, 0.047966\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1),\n#     { 'n_estimators': np.arange(100, 500, 50) },\n#     scoring = 'neg_mean_squared_error', return_train_score = True, cv = cv, n_jobs = -1,\n# )\n\n# # 2, tune max_depth, best value found: 4, 0.047581\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250),\n#     { 'max_depth': np.arange(1, 9, 1) },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )\n# # 3, tune min_child_weight, best value found: 2, 0.047548\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250, max_depth = 4),\n#     { 'min_child_weight': np.arange(1, 9, 1) },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )\n# # 4, tune gamma, best value found: 0, 0.047581\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250, max_depth = 4, min_child_weight = 1),\n#     { 'gamma': np.arange(0, 0.5, 0.1) },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )\n# # 5, tune subsample & colsample_bytree, best value found: 0.5 & 0.6,  0, 0.046886\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250, max_depth = 4, min_child_weight = 1, gamma = 0, ),\n#     { 'subsample': np.arange(0.5, 1.01,  0.1), 'colsample_bytree': np.arange(0.5, 1.01, 0.1) },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )\n# # 6, tune alpha & lambda, best value found: 0.17704 & 0.75, 0.046783\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250, max_depth = 4, min_child_weight = 1, gamma = 0, subsample = 0.5, colsample_bytree = 0.6),\n#     { 'reg_alpha': np.logspace(np.log(1e-4), np.log(1.5), base = np.exp(1), num = 10), 'reg_lambda': [.1, .25, .5, .75, .9, .95, .99, 1] },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )\n# # 7, tune learning_rate, best value found: 0.1, 0.046777\n# model = GridSearchCV(\n#     XGBRegressor(n_jobs = -1, n_estimators = 250, max_depth = 4, min_child_weight = 1, gamma = 0, subsample = 0.5, colsample_bytree = 0.6, reg_alpha = 0.17704, reg_lambda = 0.75),\n#     { 'learning_rate': [0.01, 0.025, 0.05, 0.75, 0.1] },\n#     scoring = 'neg_mean_squared_error',\n#     return_train_score = True,\n#     cv = cv,\n#     n_jobs = -1,\n# )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a00bb27f0ce98150be953b55e27ae590653a5f2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# the script below run a bayesian optimization search for parameters, take many hours\n\n# from timeit import default_timer as timer\n# import pickle\n\n# # https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0\n# from hyperopt import hp, fmin, Trials, tpe, STATUS_OK\n\n# # trials = Trials()\n# # or\n# # with open('xgb_trials.pickle', 'rb') as handle: trials = pickle.load(handle)\n\n# def objective(params):\n#     for k in params: params[k] = [params[k]]\n#     params['n_estimators'] = [int(params['n_estimators'][0])]\n#     params['max_depth'] = [int(params['max_depth'][0])]\n#     params['min_child_weight'] = [int(params['min_child_weight'][0])]    \n#     model = GridSearchCV(\n#         XGBRegressor(n_jobs = -1),\n#         params,\n#         scoring = 'neg_mean_squared_error',\n#         return_train_score = True,\n#         cv = cv,\n#         n_jobs = -1,\n#     )\n#     model.fit(X, y) \n#     result = {\n#         'loss': np.sqrt(-model.best_score_),\n#         'params': model.best_params_,\n#         'elapsed': timer() - start_time,\n#     }\n#     rounded_result = { k: round(v, 6) if type(v) != dict else { k2: round(v2, 6) for k2, v2, in v.items() } for k, v in result.items() }\n#     n_trial = len(trials.tids)\n#     print('Trial', n_trial, rounded_result)\n#     # save the trials every now and then\n#     if n_trial % 20 == 0:\n#         with open('xgb_trials.pickle', 'wb') as handle:\n#             pickle.dump(trials, handle, protocol = pickle.HIGHEST_PROTOCOL)\n    \n#     result['status'] = STATUS_OK\n#     return(result)\n\n# # set up timer & run\n# start_time = timer()\n\n# best = fmin(\n#     fn = objective,\n#     space = {\n#         'n_estimators': hp.quniform('n_estimators', 50, 500, 50),\n#         'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n#         'max_depth': hp.quniform('max_depth', 3, 10, 1),\n#         'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n#         'gamma': hp.uniform('gamma', 0, 0.4),\n#         'subsample': hp.uniform('subsample', 0.6, 0.9),\n#         'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 0.9),\n#         'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-5), np.log(1)),\n#     },\n#     algo = tpe.suggest,\n#     trials = trials,\n#     rstate = np.random.RandomState(0),\n#     max_evals = 10000,\n# )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ebf8794fecead77187ce2926c47b29a84e02ca1"
      },
      "cell_type": "markdown",
      "source": "### LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "229b5fa32145944db7cdb22e6d2a1b50cca1468f"
      },
      "cell_type": "code",
      "source": "from lightgbm import LGBMRegressor\nmodel = make_pipeline(LGBMRegressor())\nscore(model);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95f7011f269385918cba39039760b85ccb88b955"
      },
      "cell_type": "markdown",
      "source": "I have ðŸ“š a lot from several notebooks (to name a few): [Erik Bruin](https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda), [Serigne](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}